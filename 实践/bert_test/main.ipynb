{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-pretrained-bert\n",
      "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
      "Requirement already satisfied: regex in d:\\develop\\anaconda3\\envs\\jupyter\\lib\\site-packages (from pytorch-pretrained-bert) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from pytorch-pretrained-bert) (2.25.1)\n",
      "Requirement already satisfied: torch>=0.4.1 in d:\\develop\\anaconda3\\envs\\jupyter\\lib\\site-packages (from pytorch-pretrained-bert) (1.12.0)\n",
      "Requirement already satisfied: numpy in d:\\develop\\anaconda3\\envs\\jupyter\\lib\\site-packages (from pytorch-pretrained-bert) (1.21.5)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.24.49-py3-none-any.whl (132 kB)\n",
      "Requirement already satisfied: tqdm in d:\\develop\\anaconda3\\envs\\jupyter\\lib\\site-packages (from pytorch-pretrained-bert) (4.64.0)\n",
      "Requirement already satisfied: typing_extensions in d:\\develop\\anaconda3\\envs\\jupyter\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.2.0)\n",
      "Collecting botocore<1.28.0,>=1.27.49\n",
      "  Downloading botocore-1.27.49-py3-none-any.whl (9.0 MB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in d:\\develop\\anaconda3\\envs\\jupyter\\lib\\site-packages (from botocore<1.28.0,>=1.27.49->boto3->pytorch-pretrained-bert) (1.26.9)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in d:\\develop\\anaconda3\\envs\\jupyter\\lib\\site-packages (from botocore<1.28.0,>=1.27.49->boto3->pytorch-pretrained-bert) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\develop\\anaconda3\\envs\\jupyter\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.49->boto3->pytorch-pretrained-bert) (1.16.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in d:\\develop\\anaconda3\\envs\\jupyter\\lib\\site-packages (from requests->pytorch-pretrained-bert) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\develop\\anaconda3\\envs\\jupyter\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2022.6.15)\n",
      "Requirement already satisfied: colorama in d:\\develop\\anaconda3\\envs\\jupyter\\lib\\site-packages (from tqdm->pytorch-pretrained-bert) (0.4.5)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
      "Successfully installed boto3-1.24.49 botocore-1.27.49 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertModel, BertTokenizer\n",
    "import numpy as np\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 加载bert的分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('D:/Develop/workspace/datasets/ppb/bert-base-uncased-vocab.txt')\n",
    "# 加载bert模型，这个路径文件夹下有bert_config.json配置文件和model.bin模型权重文件\n",
    "bert = BertModel.from_pretrained('D:/Develop/workspace/datasets/ppb/bert-base-uncased/bert-base-uncased/')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\\'\\m\\not\\sure\\,\\this\\can\\work\\,\\lo\\##l\\-\\.\\-\n"
     ]
    }
   ],
   "source": [
    "s = \"I'm not sure, this can work, lol -.-\"\n",
    "\n",
    "tokens = tokenizer.tokenize(s)\n",
    "print(\"\\\\\".join(tokens))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15])\n"
     ]
    }
   ],
   "source": [
    "# \"i\\\\'\\\\m\\\\not\\\\sure\\\\,\\\\this\\\\can\\\\work\\\\,\\\\lo\\\\##l\\\\-\\\\.\\\\-\"\n",
    "# 是否需要这样做？\n",
    "# tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "\n",
    "ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokens)])\n",
    "print(ids.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([tensor([[[ 0.0149,  0.2906, -0.3904,  ...,  0.2831,  0.2524,  0.0099],\n",
      "         [-0.3020,  0.0948, -0.3686,  ...,  0.9025,  0.6680, -1.3179],\n",
      "         [ 0.2868,  0.6225,  0.0466,  ..., -0.2463,  0.6610, -0.7523],\n",
      "         ...,\n",
      "         [ 0.8117, -0.0816,  0.1211,  ...,  0.0701,  1.2127, -0.0546],\n",
      "         [ 0.1606,  0.3047, -0.5439,  ..., -0.4230,  0.2595,  0.4675],\n",
      "         [ 0.6798,  0.0356, -0.3259,  ...,  0.8304,  0.4200,  0.8369]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[ 0.1213, -0.1440, -0.3330,  ...,  0.3816,  0.3902, -0.3032],\n",
      "         [-0.4343,  0.2968,  0.0816,  ...,  1.3178,  1.1097, -1.8229],\n",
      "         [ 0.1688,  0.7390,  0.2408,  ...,  0.1749,  0.4786, -0.2924],\n",
      "         ...,\n",
      "         [ 1.0885,  0.3901,  0.9672,  ...,  0.6685,  1.9166, -0.2629],\n",
      "         [ 0.1387,  0.0637, -0.3391,  ..., -0.2014,  0.3827,  0.6439],\n",
      "         [ 0.6293,  0.9271,  0.5554,  ...,  1.2325,  0.2373,  0.6565]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[ 0.1505, -0.4199, -0.2121,  ...,  0.4199,  0.4271, -0.0356],\n",
      "         [-0.3127, -0.1633,  0.5921,  ...,  0.8568,  0.8379, -1.8134],\n",
      "         [ 0.4295,  0.3670,  0.1642,  ...,  0.3534, -0.3499, -0.1278],\n",
      "         ...,\n",
      "         [ 0.7539,  0.2999,  1.3020,  ...,  0.2207,  1.8181, -0.5924],\n",
      "         [-0.0985, -0.2449, -0.0528,  ..., -0.4420,  0.3060,  0.7738],\n",
      "         [ 0.8773,  0.8521,  1.2192,  ...,  0.5931,  0.2782,  0.3953]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[ 0.4595, -0.6796, -0.5673,  ...,  0.4521,  0.4113,  0.3926],\n",
      "         [-0.6905, -0.4954,  0.4347,  ...,  0.8440,  0.4218, -1.1533],\n",
      "         [ 1.0583,  0.0799,  0.3427,  ...,  0.4254,  0.3865,  0.2263],\n",
      "         ...,\n",
      "         [ 0.8600, -0.1139,  1.1693,  ...,  0.5031,  1.1591, -0.4342],\n",
      "         [-0.3465, -0.6824,  1.0061,  ...,  0.1525, -0.0365,  0.7957],\n",
      "         [ 0.4738,  0.5377,  1.2270,  ...,  0.8553, -0.5236,  0.6471]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[ 0.1653, -0.6652,  0.0135,  ...,  0.3088,  0.6290,  0.5965],\n",
      "         [-0.2277, -0.2653,  0.6250,  ...,  0.4197, -0.0228, -0.8050],\n",
      "         [ 1.1162,  0.4739, -0.1885,  ...,  0.0310,  0.4477,  0.3852],\n",
      "         ...,\n",
      "         [ 0.4510,  0.0586,  1.8314,  ...,  1.2169,  0.7438, -0.7733],\n",
      "         [-0.3931, -0.3257,  0.7296,  ..., -0.3799,  0.2021,  0.5614],\n",
      "         [ 0.3009, -0.1892,  1.2250,  ...,  0.3181,  0.7788,  0.8992]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[-0.0325, -0.2447,  0.2071,  ..., -0.0260,  0.5361,  0.6040],\n",
      "         [-0.5423, -0.1683,  0.7154,  ..., -0.0979, -0.2579, -0.6101],\n",
      "         [ 0.7010,  0.0102, -0.2150,  ..., -0.6467,  0.4582,  0.8821],\n",
      "         ...,\n",
      "         [-0.1234,  0.4420,  1.8979,  ...,  1.0391,  0.6073, -0.7084],\n",
      "         [-0.1960, -0.1590,  1.1704,  ..., -0.2553, -0.0032,  0.4320],\n",
      "         [ 0.2361, -0.0992,  1.5224,  ...,  0.6453,  0.8195,  0.6751]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[-0.3741, -0.2105,  0.9005,  ...,  0.0374,  0.5259,  0.3509],\n",
      "         [-0.5430,  0.0069,  0.7242,  ...,  0.0276,  0.4994, -0.5768],\n",
      "         [ 0.4593,  0.6452, -0.2671,  ..., -0.3296,  0.4029,  0.5250],\n",
      "         ...,\n",
      "         [-0.1631,  0.6690,  1.8991,  ...,  0.8011,  0.5928, -0.4268],\n",
      "         [-0.4149, -0.1969,  1.6881,  ..., -0.1128,  1.0631,  0.2330],\n",
      "         [-0.0237,  0.4179,  1.8417,  ...,  0.5511,  0.7043,  0.8102]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[-2.2162e-02, -1.3638e-01,  1.1845e+00,  ...,  2.2816e-01,\n",
      "           4.2894e-01,  2.0761e-01],\n",
      "         [-3.7188e-01,  4.0909e-01,  9.9852e-01,  ...,  4.0363e-01,\n",
      "           5.4164e-01, -7.7000e-01],\n",
      "         [ 7.3836e-01,  1.1046e+00, -1.2719e-01,  ...,  1.0173e-03,\n",
      "           2.8278e-01,  5.0625e-01],\n",
      "         ...,\n",
      "         [ 2.2393e-01,  9.3734e-02,  2.4183e+00,  ...,  8.0239e-01,\n",
      "          -1.4022e-01,  9.7824e-02],\n",
      "         [-2.3967e-01,  1.0202e-02,  1.9284e+00,  ...,  7.2474e-02,\n",
      "           6.6921e-01,  3.1836e-01],\n",
      "         [ 1.2094e-01, -2.7236e-01,  2.2574e+00,  ...,  2.5991e-01,\n",
      "           3.6987e-01,  8.3094e-01]]], grad_fn=<AddBackward0>), tensor([[[ 0.4393, -0.1839,  0.7901,  ...,  0.0546,  0.1462,  0.1095],\n",
      "         [-0.3134,  0.5047,  1.3247,  ..., -0.1453,  0.1982, -0.7938],\n",
      "         [ 0.7924,  1.3100,  0.0480,  ..., -0.2213,  0.1931,  0.5800],\n",
      "         ...,\n",
      "         [ 0.8776, -0.0280,  2.2334,  ...,  0.2221,  0.0777, -0.1805],\n",
      "         [ 0.1012, -0.1208,  1.4354,  ..., -0.1290,  0.4377, -0.0663],\n",
      "         [ 0.6642, -0.2577,  1.8191,  ..., -0.0604,  0.2941,  0.6703]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[ 0.3540, -0.3809,  0.9128,  ...,  0.4114, -0.0789,  0.2142],\n",
      "         [-0.1343,  0.0493,  1.2924,  ..., -0.0207,  0.3117, -0.3706],\n",
      "         [ 0.6318,  0.5846,  0.0774,  ...,  0.1859, -0.0265,  1.2382],\n",
      "         ...,\n",
      "         [ 0.8985, -0.1315,  1.7574,  ...,  0.7045, -0.0189, -0.1978],\n",
      "         [ 0.3432, -0.2165,  1.8786,  ...,  0.5192,  0.4424,  0.0878],\n",
      "         [ 0.8059, -0.3941,  1.5495,  ...,  0.6444,  0.2424,  0.8142]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[ 0.3372, -0.5063,  0.6959,  ..., -0.1344, -0.0291,  0.3771],\n",
      "         [ 0.1591,  0.2315,  1.3512,  ..., -0.6307,  0.4066, -0.1747],\n",
      "         [ 0.5299,  0.3381,  0.2253,  ..., -0.4267,  0.0710,  1.5337],\n",
      "         ...,\n",
      "         [ 0.7695,  0.2677,  1.3118,  ..., -0.0679, -0.0312, -0.0395],\n",
      "         [-0.0115, -0.5628,  1.4221,  ...,  0.4369,  0.7844, -0.1756],\n",
      "         [ 0.5155, -0.1722,  1.3973,  ...,  0.0700,  0.4681,  0.9799]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[ 0.1026,  0.0056,  0.3269,  ..., -0.2310,  0.5006,  0.3524],\n",
      "         [-0.0052,  0.2403,  0.7917,  ..., -0.1117,  0.6851, -0.1616],\n",
      "         [ 0.3617,  0.6245,  0.5025,  ..., -0.7400,  0.5553,  1.1774],\n",
      "         ...,\n",
      "         [ 0.4263,  0.2797,  0.6420,  ..., -0.1508,  0.4344,  0.0407],\n",
      "         [-0.1305,  0.0938,  0.7400,  ...,  0.0644,  0.8307,  0.3710],\n",
      "         [ 0.1375,  0.0317,  0.7568,  ..., -0.0448,  0.7929,  0.7897]]],\n",
      "       grad_fn=<AddBackward0>)], tensor([[-5.4974e-01, -4.1167e-02,  5.7031e-01,  3.4648e-01, -4.0641e-01,\n",
      "          1.4812e-02,  4.8443e-01,  1.0643e-01,  7.2033e-01, -9.7237e-01,\n",
      "          7.2275e-01, -2.3958e-01,  9.5422e-01, -5.9496e-01,  9.4891e-01,\n",
      "          3.4146e-02,  1.8115e-01, -2.6710e-01,  7.3219e-02, -1.9162e-01,\n",
      "          7.8834e-01,  7.1016e-01,  7.6436e-01,  3.1269e-02,  1.1009e-01,\n",
      "         -2.4528e-01, -1.1031e-02,  9.5057e-01,  9.1182e-01,  6.6521e-01,\n",
      "         -5.2144e-01,  1.0108e-02, -9.8449e-01, -1.9234e-02, -1.3043e-01,\n",
      "         -9.4954e-01, -1.3679e-01, -3.3880e-01, -1.3658e-01,  2.1256e-01,\n",
      "         -8.9429e-01, -2.7191e-02,  9.5342e-01, -5.3249e-01,  3.7677e-01,\n",
      "         -8.6824e-02, -8.6220e-01, -6.6446e-02, -8.8474e-01, -8.5628e-01,\n",
      "         -5.8694e-01, -7.0495e-01, -1.2323e-01,  5.9994e-02,  3.8328e-02,\n",
      "          4.5428e-01, -2.7345e-01, -2.5293e-02,  1.7185e-01, -1.1130e-01,\n",
      "         -1.4684e-01,  2.2238e-01,  3.2836e-01, -7.7693e-01, -8.6152e-01,\n",
      "         -7.7961e-01, -1.9104e-02, -4.6958e-02,  1.8409e-01, -2.2704e-01,\n",
      "          3.8651e-01,  1.0559e-01,  6.3655e-01, -9.1298e-01, -8.7105e-01,\n",
      "          5.0320e-03, -4.5427e-01,  9.7933e-01,  1.2996e-04, -9.6943e-01,\n",
      "          4.4143e-01, -8.1434e-01,  5.0146e-01,  9.0469e-01, -8.1372e-01,\n",
      "         -9.5975e-01, -1.2424e-01,  5.4140e-02, -9.8042e-01, -8.1723e-02,\n",
      "          2.7660e-01,  2.2636e-01, -1.8568e-01,  4.3098e-01,  3.5569e-01,\n",
      "         -2.9791e-02,  1.3408e-01,  5.2313e-01, -1.3073e-01,  2.1352e-01,\n",
      "          8.6496e-02,  2.8787e-02, -9.8894e-03, -1.5777e-02, -4.6970e-03,\n",
      "         -4.2490e-02, -3.6057e-01, -2.6101e-01, -6.7069e-01,  9.4870e-02,\n",
      "          2.9381e-01, -5.5589e-03, -1.8751e-02, -8.7927e-01,  5.1220e-02,\n",
      "          2.1696e-02, -9.6763e-01, -3.5759e-01, -9.8588e-01,  6.3364e-01,\n",
      "          7.6124e-03, -1.6644e-01,  9.4194e-01,  2.4273e-01, -1.0715e-01,\n",
      "          3.6760e-02,  8.6367e-01, -9.8732e-01,  3.1449e-01, -3.0185e-01,\n",
      "          4.5167e-01,  5.5119e-02, -9.6189e-01, -9.6013e-01,  1.8480e-01,\n",
      "          8.5784e-01,  6.9830e-04,  9.5758e-01, -1.1698e-01,  9.2677e-01,\n",
      "          8.0421e-01,  3.8179e-01, -7.3093e-01, -1.4007e-01,  3.5930e-01,\n",
      "          1.8553e-02, -2.5311e-01, -2.0124e-01,  2.2304e-01, -5.0692e-01,\n",
      "         -3.0068e-02,  1.4238e-01,  6.9685e-01, -9.1098e-01, -4.0156e-02,\n",
      "          9.5755e-01,  3.1373e-01,  8.5517e-01,  8.6866e-01,  1.3493e-01,\n",
      "         -1.6530e-01,  6.3075e-01,  1.4130e-01,  1.8316e-02,  3.1660e-01,\n",
      "          1.5800e-01, -7.9512e-01,  1.3345e-01, -5.7945e-01,  7.1538e-01,\n",
      "          1.7048e-02,  1.0529e-01,  4.2412e-01, -9.6741e-01,  1.1747e-01,\n",
      "          2.4735e-01,  9.7635e-01,  5.7216e-01,  4.7427e-02, -7.2045e-01,\n",
      "         -6.6130e-02, -3.5097e-01, -9.4365e-01,  9.6855e-01,  3.4403e-02,\n",
      "          3.9694e-03, -2.2513e-01, -3.4530e-01, -7.0910e-01, -4.7297e-01,\n",
      "          3.8119e-01,  5.3152e-01, -7.1784e-01,  5.9902e-02, -1.9994e-01,\n",
      "         -8.3756e-02, -1.3870e-01,  7.1132e-02, -6.8970e-02, -2.0150e-01,\n",
      "         -2.4994e-01,  9.1876e-01,  3.0144e-01,  4.9280e-01, -6.0062e-01,\n",
      "          1.9663e-01, -8.2170e-01, -5.6281e-01, -1.3094e-01, -1.0851e-01,\n",
      "         -3.7085e-02,  9.7583e-01, -3.3457e-01,  2.0820e-01, -8.1893e-01,\n",
      "         -9.7322e-01, -1.9934e-01, -7.6730e-01,  1.9335e-01, -2.7937e-01,\n",
      "          6.5451e-02,  2.6093e-01, -8.7845e-01,  2.1465e-01, -7.5752e-01,\n",
      "         -7.6926e-01,  4.5642e-02, -1.1897e-01,  7.6826e-02,  2.4913e-02,\n",
      "          9.1918e-01, -2.5674e-01, -3.2119e-01, -2.1591e-02,  9.4926e-01,\n",
      "         -7.5225e-02, -7.7378e-01,  7.0358e-01,  1.5990e-01,  4.5907e-01,\n",
      "         -2.0483e-01,  8.5833e-01, -3.5774e-01,  6.8528e-02, -8.8335e-01,\n",
      "          3.2175e-01,  1.0410e-02,  7.9045e-01, -4.9389e-02, -8.4234e-01,\n",
      "         -8.5234e-01,  4.1477e-01, -7.1541e-02,  8.6055e-01, -1.2147e-01,\n",
      "          4.7936e-01, -9.2686e-01, -9.4857e-01, -6.1818e-01,  5.6037e-01,\n",
      "         -9.8502e-01,  6.4461e-02,  1.6425e-01,  1.7774e-01, -7.0232e-02,\n",
      "         -1.6756e-01, -9.6156e-01,  3.3767e-01, -4.0191e-03,  6.9229e-01,\n",
      "         -2.9780e-02, -4.6483e-01,  4.5033e-01, -9.4755e-01, -2.0197e-01,\n",
      "          8.1146e-02,  8.7548e-01, -3.0826e-01, -9.0157e-01,  7.5622e-02,\n",
      "          3.8465e-01,  7.2214e-02,  8.0850e-01,  7.3462e-01,  9.0141e-01,\n",
      "          9.4392e-01,  8.2653e-01,  4.4166e-01, -6.9848e-01, -2.9963e-01,\n",
      "          9.7983e-01,  4.8603e-02, -9.7077e-01, -8.6723e-01, -4.6168e-02,\n",
      "          3.1060e-01, -9.8383e-01,  6.3234e-03,  1.6073e-01, -8.3949e-01,\n",
      "         -6.9642e-01,  9.2408e-01,  7.5047e-01, -9.7847e-01,  4.5783e-01,\n",
      "          8.6401e-01, -4.5542e-01, -5.5698e-01, -2.7199e-02,  9.4234e-01,\n",
      "         -1.1507e-01,  3.2057e-01, -4.7999e-03,  1.9387e-01,  3.6504e-01,\n",
      "         -5.7272e-01,  6.5554e-01,  2.2383e-01,  2.3736e-01, -6.1283e-02,\n",
      "         -3.2476e-01, -8.9923e-01, -2.3411e-01,  3.7845e-02, -2.3721e-01,\n",
      "         -9.6330e-01, -1.8610e-02, -6.3853e-01,  3.2070e-01, -5.9226e-04,\n",
      "          2.2445e-02, -5.0019e-01, -1.1644e-01, -5.4656e-01,  2.7252e-01,\n",
      "          4.0444e-01, -7.3399e-01, -2.5456e-01,  5.0591e-01, -7.7435e-01,\n",
      "          6.3242e-01, -9.6152e-01,  9.4034e-01, -1.8423e-01, -8.2612e-01,\n",
      "          9.8674e-01, -1.4309e-01, -8.5244e-01,  1.8043e-02, -9.7617e-02,\n",
      "         -5.9775e-01,  9.6209e-01,  9.8771e-02, -9.7110e-01, -4.5817e-01,\n",
      "         -2.1284e-01, -7.9389e-02, -1.1415e-01,  9.6291e-01,  1.5900e-01,\n",
      "          8.3774e-01,  5.6231e-01,  9.8129e-01, -9.7826e-01, -7.4853e-03,\n",
      "         -6.1415e-01, -9.4547e-01,  9.3229e-01,  9.4698e-01,  4.2171e-02,\n",
      "         -4.8202e-01, -1.8840e-01,  6.4921e-01, -3.8936e-02, -5.2288e-01,\n",
      "          2.0957e-01,  2.8547e-01,  6.6071e-02,  8.9723e-01, -1.0464e-02,\n",
      "         -3.8080e-01, -1.8533e-01,  6.7492e-01,  7.3579e-01,  2.0836e-01,\n",
      "          1.6280e-01, -1.3919e-01, -1.6779e-01, -5.3572e-02, -6.6699e-01,\n",
      "         -9.1894e-01,  1.1694e-01,  9.8155e-01,  3.3789e-01,  1.1840e-01,\n",
      "          7.8504e-01,  5.3528e-02, -5.2544e-02, -4.4978e-02,  1.3771e-01,\n",
      "          6.4195e-02, -5.4895e-01, -4.1271e-01, -6.9579e-01, -9.8186e-01,\n",
      "          7.0107e-01,  4.3383e-02, -1.3070e-02,  8.0900e-01, -3.9267e-01,\n",
      "         -9.6721e-02, -4.3564e-01, -6.0322e-01, -6.1381e-02,  1.1579e-01,\n",
      "         -9.0718e-01,  9.5888e-01,  6.6257e-02,  4.7858e-01,  4.1771e-01,\n",
      "          8.9569e-01, -3.8718e-01, -1.9325e-01, -1.4311e-01, -9.3401e-01,\n",
      "          9.1468e-02, -9.3930e-01,  9.5448e-01, -7.8109e-01, -5.5773e-03,\n",
      "         -2.6253e-02, -6.0100e-02,  9.6568e-01, -6.6522e-01,  4.2163e-01,\n",
      "          5.7687e-01,  6.3387e-01, -5.9045e-01, -7.1826e-01, -1.5994e-01,\n",
      "          2.8710e-01,  8.4768e-01, -3.0556e-02, -4.3903e-02, -9.0255e-01,\n",
      "         -8.6173e-01, -2.7513e-01, -6.7718e-01, -9.7225e-01,  6.2399e-01,\n",
      "         -1.2254e-01, -1.9279e-01, -4.1920e-01, -7.1272e-02, -4.8644e-01,\n",
      "         -5.5150e-01,  1.5151e-01, -9.1466e-01,  8.8142e-01, -5.8999e-02,\n",
      "          6.4284e-02,  3.1773e-02,  3.8484e-01, -7.6647e-01,  9.0898e-01,\n",
      "          1.6999e-01,  1.1695e-01, -1.0692e-01, -5.6548e-01,  3.1193e-01,\n",
      "         -3.6357e-01,  3.1403e-01,  2.5764e-02,  9.8259e-01, -4.1795e-01,\n",
      "         -3.6297e-01,  6.5121e-01,  2.0997e-01,  1.1336e-01,  1.7089e-01,\n",
      "          1.5517e-01,  2.3659e-01,  7.8937e-01,  8.8980e-01,  4.5734e-01,\n",
      "          9.7813e-03,  2.5360e-01, -3.7955e-02, -6.6335e-01,  8.2543e-01,\n",
      "         -1.4790e-01, -5.8046e-02,  1.9092e-01,  4.1643e-02,  3.8532e-01,\n",
      "          9.8437e-02, -2.5853e-02, -2.0974e-01, -5.9572e-02, -1.2674e-01,\n",
      "          8.7053e-02,  9.7124e-01,  2.8490e-02, -2.2575e-01, -9.7967e-01,\n",
      "          4.0747e-01, -4.0371e-01,  8.9272e-01,  8.5847e-01, -3.0300e-01,\n",
      "          1.9223e-01, -4.3223e-02,  1.0532e-01,  8.2738e-02,  9.0656e-02,\n",
      "          5.2422e-02, -5.2193e-02,  6.2398e-02,  9.3012e-01, -1.6345e-01,\n",
      "         -9.6847e-01, -5.6640e-01, -1.8553e-02, -9.2993e-01,  6.6996e-01,\n",
      "         -5.9053e-02, -4.5091e-02, -2.2592e-02,  3.8688e-01, -2.8573e-01,\n",
      "         -8.1161e-02, -9.6641e-01, -5.2944e-02, -1.4258e-02,  9.6514e-01,\n",
      "         -8.8866e-02, -4.0866e-01, -8.8918e-01, -7.5515e-01, -4.0955e-01,\n",
      "          6.6441e-01, -9.0107e-01,  9.6828e-01, -9.5328e-01, -1.9885e-01,\n",
      "          9.2988e-01, -3.1844e-02, -8.8225e-01, -4.5301e-02, -3.5788e-02,\n",
      "          1.3224e-03,  1.4303e-01,  3.7407e-01, -9.5896e-01,  7.8151e-03,\n",
      "          3.7447e-02, -7.7445e-02,  1.6326e-01, -1.8568e-01,  6.7736e-01,\n",
      "          1.2057e-01, -3.8966e-01, -1.0102e-01,  5.9366e-02,  6.1248e-02,\n",
      "          5.0253e-01, -1.2378e-01,  1.0713e-01, -1.0915e-01,  8.9191e-02,\n",
      "         -9.2908e-01,  1.2837e-01, -6.1615e-02, -7.5147e-01,  4.0734e-01,\n",
      "         -9.7815e-01, -2.1522e-01, -8.3250e-01, -4.5588e-02,  8.8101e-01,\n",
      "          3.8851e-01, -1.5839e-01, -7.0339e-01,  8.8271e-01,  9.5567e-01,\n",
      "          7.2320e-01, -1.3675e-01,  7.8771e-01, -7.8248e-01,  7.8385e-02,\n",
      "         -3.1590e-02,  6.3369e-02,  3.6052e-01,  5.2687e-01,  2.9026e-02,\n",
      "          9.8620e-01, -9.4311e-02, -2.3082e-01, -7.2584e-01, -1.2528e-02,\n",
      "         -4.2764e-03,  9.0472e-01, -1.1550e-01, -9.4582e-01,  2.2651e-01,\n",
      "         -1.8416e-01, -8.6343e-01,  5.8928e-02, -5.9657e-02, -1.7599e-01,\n",
      "          1.0773e-01,  9.4896e-01,  2.1877e-01, -4.1836e-01,  3.0384e-01,\n",
      "         -4.3514e-02,  4.5113e-02, -6.6172e-02, -5.4569e-01,  9.7761e-01,\n",
      "          1.0729e-01,  3.7622e-01,  5.6250e-02, -1.6233e-01,  9.6182e-01,\n",
      "          2.0010e-01,  7.9637e-02, -3.3931e-02,  9.5872e-01,  1.0632e-01,\n",
      "         -8.7976e-01,  1.8527e-01, -8.9160e-01,  6.1350e-03, -8.0250e-01,\n",
      "         -4.3988e-02,  1.4164e-01,  8.2233e-01,  1.2555e-01,  9.2039e-01,\n",
      "          7.4146e-01, -1.1193e-01,  5.8889e-01,  8.2706e-01,  1.5328e-01,\n",
      "         -9.3602e-01, -9.6043e-01, -9.7407e-01, -2.4564e-01, -7.8744e-02,\n",
      "          3.8227e-02, -1.5318e-02, -9.4120e-02, -2.0245e-01,  1.3207e-02,\n",
      "         -9.6607e-01,  8.4262e-01,  1.4757e-01, -7.1878e-01,  9.4881e-01,\n",
      "          1.3989e-01, -1.3157e-01, -4.3960e-02, -9.7166e-01, -7.7360e-01,\n",
      "          1.0531e-01,  7.2393e-02,  5.5431e-01,  1.1708e-01,  8.3465e-01,\n",
      "         -6.8256e-02, -1.0286e-01, -2.5050e-01,  4.2026e-01, -6.3766e-01,\n",
      "         -9.8274e-01,  1.9589e-01,  9.0406e-01, -5.5515e-01,  9.6649e-01,\n",
      "         -8.1468e-01, -6.6777e-02,  7.7207e-01,  2.5155e-01,  1.7273e-01,\n",
      "          5.1449e-01,  3.1554e-01, -1.6401e-01,  5.8289e-01,  8.6661e-01,\n",
      "          7.1396e-01,  9.6399e-01,  6.8845e-01,  3.2849e-01,  7.2574e-01,\n",
      "          1.1295e-01,  7.6959e-01, -8.6947e-01, -6.7645e-02,  1.6317e-01,\n",
      "          1.5049e-01,  4.6615e-02,  4.7744e-02, -6.2010e-01,  4.6422e-01,\n",
      "          4.8125e-05,  3.1261e-01, -1.8373e-01,  7.1698e-02, -1.0152e-01,\n",
      "          1.1208e-02, -4.3816e-01, -3.7795e-02,  5.1909e-01, -3.6383e-02,\n",
      "          8.9378e-01,  3.9125e-02,  7.9477e-02, -2.3488e-02,  2.0822e-01,\n",
      "          8.3619e-01, -8.4965e-01,  2.8613e-01,  1.3152e-01,  8.1330e-01,\n",
      "         -5.8326e-02, -1.7417e-01,  7.9640e-01, -7.1385e-01,  1.1073e-01,\n",
      "          1.4343e-01, -5.7157e-02,  5.9617e-01, -2.4169e-01, -6.0245e-02,\n",
      "         -2.6083e-01,  2.4104e-01,  4.8632e-02,  7.5574e-01,  6.7239e-01,\n",
      "          9.0100e-01,  1.7201e-01,  1.7305e-02,  1.8772e-01, -2.8023e-01,\n",
      "         -9.6191e-01,  1.2618e-01,  6.4589e-01, -3.8030e-01,  5.9106e-01,\n",
      "         -2.0470e-01,  6.6872e-01, -8.8112e-01, -7.8226e-02,  3.2495e-01,\n",
      "         -6.0412e-01,  4.2565e-02,  4.7287e-01,  4.1980e-01,  9.2164e-01,\n",
      "         -4.5303e-01,  7.7828e-01,  6.6300e-01,  7.8718e-01,  4.1946e-01,\n",
      "          3.7778e-01, -2.9321e-01,  8.5400e-01]], grad_fn=<TanhBackward0>))\n"
     ]
    }
   ],
   "source": [
    "result = bert(ids, output_all_encoded_layers=True)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from torch import  nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_path, n_other_features, n_hidden):\n",
    "        super().__init__()\n",
    "        # 加载并冻结bert模型参数\n",
    "        self.bert = BertModel.from_pretrained(bert_path)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(768 + n_other_features, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, 1)\n",
    "        )\n",
    "    def forward(self, seqs, features):\n",
    "        _, pooled = self.bert(seqs, output_all_encoded_layers=False)\n",
    "        concat = torch.cat([pooled, features], dim=1)\n",
    "        logits = self.output(concat)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0491]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "s = \"I'm not sure, this can work, lol -.-\"\n",
    "\n",
    "tokens = tokenizer.tokenize(s)\n",
    "ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokens)])\n",
    "\n",
    "model = CustomModel('D:/Develop/workspace/datasets/ppb/bert-base-uncased/bert-base-uncased/',10, 512)\n",
    "outputs = model(ids, torch.rand(1, 10))\n",
    "print(outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}