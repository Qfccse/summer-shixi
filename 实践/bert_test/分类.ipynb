{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d499cd43-2143-4f2b-8148-6f808f854d0b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json, time \n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, BertConfig, BertTokenizer, AdamW, get_cosine_schedule_with_warmup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe2c89-4bde-46bb-b04e-f38a27d73103",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "bert_path = \"D:/Develop/workspace/datasets/bert/chinese_wwm_ext_pytorch/\"    # 该文件夹下存放三个文件（'vocab.txt', 'pytorch_model.bin', 'config.json'）\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_path)   # 初始化分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a56f730-a018-48c6-a1b5-4043816d6d6e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "input_ids, input_masks, input_types,  = [], [], []  # input char ids, segment type ids,  attention mask\n",
    "labels = []         # 标签\n",
    "maxlen = 30      # 取30即可覆盖99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ae600c-a84b-4380-8571-c4257ae32925",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "with open( \"/data/news_title_dataset.csv\", encoding='utf-8') as f:\n",
    "    for i, line in tqdm(enumerate(f)): \n",
    "        title, y = line.strip().split('\\t')\n",
    "\n",
    "        # encode_plus会输出一个字典，分别为'input_ids', 'token_type_ids', 'attention_mask'对应的编码\n",
    "        # 根据参数会短则补齐，长则切断\n",
    "        encode_dict = tokenizer.encode_plus(text=title, max_length=maxlen, \n",
    "                                            padding='max_length', truncation=True)\n",
    "        \n",
    "        input_ids.append(encode_dict['input_ids'])\n",
    "        input_types.append(encode_dict['token_type_ids'])\n",
    "        input_masks.append(encode_dict['attention_mask'])\n",
    "\n",
    "        labels.append(int(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f8bf3-3b84-4abc-86c6-e15d89630950",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "input_ids, input_types, input_masks = np.array(input_ids), np.array(input_types), np.array(input_masks)\n",
    "labels = np.array(labels)\n",
    "print(input_ids.shape, input_types.shape, input_masks.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27eaecbb-eb01-4627-8a5b-896e7140e9d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000,) [84968 96544   870 71358 89287 95337 74539 26224 80363 15792]\n",
      "(80000, 30) (80000,) (10000, 30) (10000,) (10000, 30) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# 随机打乱索引\n",
    "idxes = np.arange(input_ids.shape[0])\n",
    "np.random.seed(2022)   # 固定种子\n",
    "np.random.shuffle(idxes)\n",
    "print(idxes.shape, idxes[:10])\n",
    "\n",
    "\n",
    "# 8:1:1 划分训练集、验证集、测试集\n",
    "input_ids_train, input_ids_valid, input_ids_test = input_ids[idxes[:80000]], input_ids[idxes[80000:90000]], input_ids[idxes[90000:]]\n",
    "input_masks_train, input_masks_valid, input_masks_test = input_masks[idxes[:80000]], input_masks[idxes[80000:90000]], input_masks[idxes[90000:]] \n",
    "input_types_train, input_types_valid, input_types_test = input_types[idxes[:80000]], input_types[idxes[80000:90000]], input_types[idxes[90000:]]\n",
    "\n",
    "y_train, y_valid, y_test = labels[idxes[:80000]], labels[idxes[80000:90000]], labels[idxes[90000:]]\n",
    "\n",
    "print(input_ids_train.shape, y_train.shape, input_ids_valid.shape, y_valid.shape, \n",
    "      input_ids_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0de5090-9e43-4ec3-8ce4-b67fb71b208a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  # 如果会出现OOM问题，减小它\n",
    "# 训练集\n",
    "train_data = TensorDataset(torch.LongTensor(input_ids_train), \n",
    "                           torch.LongTensor(input_masks_train), \n",
    "                           torch.LongTensor(input_types_train), \n",
    "                           torch.LongTensor(y_train))\n",
    "train_sampler = RandomSampler(train_data)  \n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 验证集\n",
    "valid_data = TensorDataset(torch.LongTensor(input_ids_valid), \n",
    "                          torch.LongTensor(input_masks_valid),\n",
    "                          torch.LongTensor(input_types_valid), \n",
    "                          torch.LongTensor(y_valid))\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_loader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 测试集（是没有标签的）\n",
    "test_data = TensorDataset(torch.LongTensor(input_ids_test), \n",
    "                          torch.LongTensor(input_masks_test),\n",
    "                          torch.LongTensor(input_types_test))\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e54550f-fe6d-488c-aa9b-64842f7051ac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 定义model\n",
    "class Bert_Model(nn.Module):\n",
    "    def __init__(self, bert_path, classes=10):\n",
    "        super(Bert_Model, self).__init__()\n",
    "        self.config = BertConfig.from_pretrained(bert_path)  # 导入模型超参数\n",
    "        self.bert = BertModel.from_pretrained(bert_path)     # 加载预训练模型权重\n",
    "        self.fc = nn.Linear(self.config.hidden_size, classes)  # 直接分类\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        out_pool = outputs[1]   # 池化后的输出 [bs, config.hidden_size]\n",
    "        logit = self.fc(out_pool)   #  [bs, classes]\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a3d221b-0b16-4976-8daa-f20f675fdb1d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_parameter_number(model):\n",
    "    #  打印模型参数量\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return 'Total parameters: {}, Trainable parameters: {}'.format(total_num, trainable_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5994d09-e35d-49ab-ad55-d41ba262407f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Bert_Model' has no attribute 'from_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_74/1804657957.py\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mBert_Model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbert_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0muse_auth_token\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m: type object 'Bert_Model' has no attribute 'from_pretrained'"
     ]
    }
   ],
   "source": [
    "# Bert_Model(bert_path, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df289b0a-e4a4-4673-80c6-706d349608dc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /mnt/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 102275338, Trainable parameters: 102275338\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 5\n",
    "model = Bert_Model('/mnt/').to(DEVICE)\n",
    "print(get_parameter_number(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f76bed13-f8d2-43ca-9126-85693ed2d227",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4) #AdamW优化器\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=len(train_loader),\n",
    "                                            num_training_steps=EPOCHS*len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab47e5b7-f795-4830-97a6-94ffe911c414",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 评估模型性能，在验证集上\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_true, val_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, att, tpe, y) in (enumerate(data_loader)):\n",
    "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "            val_true.extend(y.squeeze().cpu().numpy().tolist())\n",
    "    \n",
    "    return accuracy_score(val_true, val_pred)  #返回accuracy\n",
    "\n",
    "\n",
    "# 测试集没有标签，需要预测提交\n",
    "def predict(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_pred = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (ids, att, tpe) in tqdm(enumerate(data_loader)):\n",
    "            y_pred = model(ids.to(device), att.to(device), tpe.to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).detach().cpu().numpy().tolist()\n",
    "            val_pred.extend(y_pred)\n",
    "    return val_pred\n",
    "\n",
    "\n",
    "def train_and_eval(model, train_loader, valid_loader, \n",
    "                   optimizer, scheduler, device, epoch):\n",
    "    best_acc = 0.0\n",
    "    patience = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for i in range(epoch):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        print(\"***** Running training epoch {} *****\".format(i+1))\n",
    "        train_loss_sum = 0.0\n",
    "        for idx, (ids, att, tpe, y) in enumerate(train_loader):\n",
    "            ids, att, tpe, y = ids.to(device), att.to(device), tpe.to(device), y.to(device)  \n",
    "            y_pred = model(ids, att, tpe)\n",
    "            loss = criterion(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()   # 学习率变化\n",
    "            \n",
    "            train_loss_sum += loss.item()\n",
    "            if (idx + 1) % (len(train_loader)//5) == 0:    # 只打印五次结果\n",
    "                print(\"Epoch {:04d} | Step {:04d}/{:04d} | Loss {:.4f} | Time {:.4f}\".format(\n",
    "                          i+1, idx+1, len(train_loader), train_loss_sum/(idx+1), time.time() - start))\n",
    "                # print(\"Learning rate = {}\".format(optimizer.state_dict()['param_groups'][0]['lr']))\n",
    "\n",
    "        \"\"\"验证模型\"\"\"\n",
    "        model.eval()\n",
    "        acc = evaluate(model, valid_loader, device)  # 验证模型的性能\n",
    "        ## 保存最优模型\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"best_bert_model.pth\") \n",
    "        \n",
    "        print(\"current acc is {:.4f}, best acc is {:.4f}\".format(acc, best_acc))\n",
    "        print(\"time costed = {}s \\n\".format(round(time.time() - start, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "686360a1-1569-4ba6-830b-616e38921369",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training epoch 1 *****\n",
      "Epoch 0001 | Step 0250/1250 | Loss 1.8034 | Time 20.1607\n",
      "Epoch 0001 | Step 0500/1250 | Loss 1.0851 | Time 40.1243\n",
      "Epoch 0001 | Step 0750/1250 | Loss 0.7919 | Time 60.4326\n",
      "Epoch 0001 | Step 1000/1250 | Loss 0.6377 | Time 80.4854\n",
      "Epoch 0001 | Step 1250/1250 | Loss 0.5429 | Time 100.5202\n",
      "current acc is 0.9628, best acc is 0.9628\n",
      "time costed = 104.20602s \n",
      "\n",
      "***** Running training epoch 2 *****\n",
      "Epoch 0002 | Step 0250/1250 | Loss 0.1210 | Time 18.9101\n",
      "Epoch 0002 | Step 0500/1250 | Loss 0.1193 | Time 38.0054\n",
      "Epoch 0002 | Step 0750/1250 | Loss 0.1189 | Time 59.0198\n",
      "Epoch 0002 | Step 1000/1250 | Loss 0.1156 | Time 78.9628\n",
      "Epoch 0002 | Step 1250/1250 | Loss 0.1144 | Time 99.0157\n",
      "current acc is 0.9660, best acc is 0.9660\n",
      "time costed = 103.47689s \n",
      "\n",
      "***** Running training epoch 3 *****\n",
      "Epoch 0003 | Step 0250/1250 | Loss 0.0632 | Time 18.6896\n",
      "Epoch 0003 | Step 0500/1250 | Loss 0.0632 | Time 38.7569\n",
      "Epoch 0003 | Step 0750/1250 | Loss 0.0632 | Time 58.8202\n",
      "Epoch 0003 | Step 1000/1250 | Loss 0.0619 | Time 80.8863\n",
      "Epoch 0003 | Step 1250/1250 | Loss 0.0608 | Time 101.4449\n",
      "current acc is 0.9700, best acc is 0.9700\n",
      "time costed = 107.87438s \n",
      "\n",
      "***** Running training epoch 4 *****\n",
      "Epoch 0004 | Step 0250/1250 | Loss 0.0326 | Time 19.9880\n",
      "Epoch 0004 | Step 0500/1250 | Loss 0.0320 | Time 40.0201\n",
      "Epoch 0004 | Step 0750/1250 | Loss 0.0334 | Time 59.9266\n",
      "Epoch 0004 | Step 1000/1250 | Loss 0.0323 | Time 79.7126\n",
      "Epoch 0004 | Step 1250/1250 | Loss 0.0318 | Time 99.4847\n",
      "current acc is 0.9715, best acc is 0.9715\n",
      "time costed = 103.94927s \n",
      "\n",
      "***** Running training epoch 5 *****\n",
      "Epoch 0005 | Step 0250/1250 | Loss 0.0176 | Time 18.5757\n",
      "Epoch 0005 | Step 0500/1250 | Loss 0.0187 | Time 38.2346\n",
      "Epoch 0005 | Step 0750/1250 | Loss 0.0184 | Time 58.0056\n",
      "Epoch 0005 | Step 1000/1250 | Loss 0.0189 | Time 77.8278\n",
      "Epoch 0005 | Step 1250/1250 | Loss 0.0187 | Time 97.6152\n",
      "current acc is 0.9733, best acc is 0.9733\n",
      "time costed = 101.32804s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_eval(model, train_loader, valid_loader, optimizer, scheduler, DEVICE, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f5d8ac2-5d9a-4451-a560-5aae8d1cc99d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "157it [00:03, 49.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Accuracy = 0.9715 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9936    0.9989    0.9963       937\n",
      "           1     0.9745    0.9886    0.9815       965\n",
      "           2     0.9840    0.9715    0.9777       949\n",
      "           3     0.9668    0.9568    0.9618       973\n",
      "           4     0.9600    0.9786    0.9692       982\n",
      "           5     0.9903    0.9808    0.9855      1041\n",
      "           6     0.9488    0.9705    0.9595      1050\n",
      "           7     0.9766    0.9616    0.9690      1041\n",
      "           8     0.9707    0.9552    0.9629      1005\n",
      "           9     0.9537    0.9555    0.9546      1057\n",
      "\n",
      "    accuracy                         0.9715     10000\n",
      "   macro avg     0.9719    0.9718    0.9718     10000\n",
      "weighted avg     0.9716    0.9715    0.9715     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载最优权重对测试集测试\n",
    "model.load_state_dict(torch.load(\"best_bert_model.pth\"))\n",
    "pred_test = predict(model, test_loader, DEVICE)\n",
    "print(\"\\n Test Accuracy = {} \\n\".format(accuracy_score(y_test, pred_test)))\n",
    "print(classification_report(y_test, pred_test, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}